{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ITEMS\n",
      "    1: Banana\n",
      "    2: Eggs\n",
      "    3: Milk\n",
      "    4: Tea\n",
      "    5: Bread\n",
      "\n",
      "\n",
      "Frequent Itemsets...\n",
      "Itemset: ('3', '2', '5'), support value: 2\n",
      "Itemset: ('3', '1', '5'), support value: 2\n",
      "Rules for itemset - ('3', '2', '5')\n",
      "Antecedents -->  Consequents --- Confidence\n",
      "('2',) --> ('3', '2', '5') --- 0.67\n",
      "('3', '2') --> ('3', '2', '5') --- 1.0\n",
      "('3', '5') --> ('3', '2', '5') --- 0.67\n",
      "('2', '5') --> ('3', '2', '5') --- 0.67\n",
      "\n",
      "Rules for itemset - ('3', '1', '5')\n",
      "Antecedents -->  Consequents --- Confidence\n",
      "('1',) --> ('3', '1', '5') --- 0.67\n",
      "('3', '1') --> ('3', '1', '5') --- 0.67\n",
      "('3', '5') --> ('3', '1', '5') --- 0.67\n",
      "('1', '5') --> ('3', '1', '5') --- 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apriori algorithm developed and used on a sample dataset\n",
    "# This code was written just for demonstration and learning purposes\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "# transactions = {\n",
    "#     1: [\"a\", \"c\", \"d\"],\n",
    "#     2: [\"b\", \"c\", \"e\"],\n",
    "#     3: [\"a\", \"b\", \"c\", \"e\"],\n",
    "#     5: [\"b\", \"e\"],\n",
    "#     6: [\"a\", \"c\", \"e\"]\n",
    "# }\n",
    "\n",
    "\n",
    "transactions = {\n",
    "    1: [\"1\", \"3\", \"4\"],\n",
    "    2: [\"2\", \"3\", \"5\"],\n",
    "    3: [\"1\", \"2\", \"3\", \"5\"],\n",
    "    5: [\"2\", \"5\"],\n",
    "    6: [\"1\", \"3\", \"5\"]\n",
    "}\n",
    "\n",
    "min_support_count = 2\n",
    "min_confidence_value = 0.6\n",
    "\n",
    "\n",
    "# apriori pruning concept\n",
    "def _pruning(current, previous, size):\n",
    "    final_keys = []\n",
    "    previous = [tuple(i) for i in previous]\n",
    "    for key in current:\n",
    "        FLAG = False\n",
    "        current_comb = list(combinations(key, size))\n",
    "        for i in current_comb:\n",
    "            if i in previous or i[::-1] in previous:\n",
    "                FLAG = True\n",
    "            else:\n",
    "                FLAG = False\n",
    "                break\n",
    "\n",
    "        if FLAG:\n",
    "            final_keys.append(key)\n",
    "\n",
    "    return final_keys\n",
    "\n",
    "\n",
    "def support_value(itemset_keys_, transactions):\n",
    "    itemset = {key: 0 for key in itemset_keys_}\n",
    "\n",
    "    for keys in itemset_keys_:\n",
    "        for val in transactions.values():\n",
    "            if set(keys) & set(val) == set(keys):\n",
    "                itemset[keys] += 1\n",
    "    return itemset\n",
    "\n",
    "\n",
    "# creating frequent itemset\n",
    "def get_frequent_itemset(size=None, transactions=None, itemset=None):\n",
    "    if size == 1:\n",
    "        itemset = Counter()\n",
    "\n",
    "        for val in transactions.values():\n",
    "            itemset.update(val)\n",
    "\n",
    "    else:\n",
    "\n",
    "        prev_itemset_keys = list(itemset.keys())\n",
    "        prev_itemset = itemset.copy()\n",
    "\n",
    "        valid_keys = list(set(itemset.keys()))\n",
    "        # flatten list of tuple -> keys: [(), ()] -> []\n",
    "        # useful for running a combination of all the chosen features\n",
    "        l = []\n",
    "        for row in valid_keys:\n",
    "            l.extend(row)\n",
    "\n",
    "        valid_keys = set(l)\n",
    "\n",
    "        # candidate itemset keys\n",
    "        itemset_keys_ = list(combinations(valid_keys, size))\n",
    "\n",
    "        # Apriori algorithm is based on theconcept that a subset\n",
    "        # of a frequent itemset must also be a frequent itemset\n",
    "        # so we are pruning away those features whose subset are not present\n",
    "        # in the previous frequent itemset\n",
    "        if size >= 2:\n",
    "            itemset_keys_ = _pruning(\n",
    "                itemset_keys_, prev_itemset_keys, size - 1)\n",
    "\n",
    "        # finding support value for each of the selected itemset feature combination\n",
    "        itemset = support_value(itemset_keys_, transactions)\n",
    "\n",
    "        # defaulting back to th previous frequent itemset if\n",
    "        # the iteration doesn't find any itemset which has the theshold required\n",
    "        if itemset == {}:\n",
    "            itemset = prev_itemset\n",
    "\n",
    "    # getting frequent itemset from itemset\n",
    "    # Frequent Itemset is an itemset whose support\n",
    "    # value is greater than a threshold value(support).\n",
    "\n",
    "    frequent_itemset = {}\n",
    "    for key, val in itemset.items():\n",
    "        if val >= min_support_count:\n",
    "            frequent_itemset[key] = val\n",
    "\n",
    "    return frequent_itemset\n",
    "\n",
    "\n",
    "def finding_subsets(frequent_set):\n",
    "    item_list = []\n",
    "    size = len(list(frequent_set.keys())[0])\n",
    "    for key in frequent_set.keys():\n",
    "        subsets = []\n",
    "        for i in range(1, size):\n",
    "            subsets.append(list(combinations(key, i)))\n",
    "\n",
    "        #subsets = list(np.array(subsets).flatten())\n",
    "               #the above line give \"setting an array element with a sequence\" error.\n",
    "        subsets = [item for sublist in subsets for item in sublist]  \n",
    "        #this list comprehension will flaten the list of subsets, and you won't encounter the \"Setting an array element with a sequence\" error\n",
    "        subsets.insert(0, key)\n",
    "        item_list.append(subsets)\n",
    "\n",
    "    return item_list\n",
    "\n",
    "\n",
    "def finding_rules(itemset_sub):\n",
    "    print(\"Antecedents -->  Consequents --- Confidence\")\n",
    "    for i in range(1, len(itemset_sub)):\n",
    "\n",
    "        # passing as list as we have designed support_value function as\n",
    "        # a function that takes an iteratable list of itemsets\n",
    "        x = support_value([itemset_sub[0], ], transactions)\n",
    "        y = support_value([itemset_sub[i], ], transactions)\n",
    "        confidence = list(x.values())[0] / list(y.values())[0]\n",
    "        if confidence >= min_confidence_value:\n",
    "            print(\n",
    "                f\"{itemset_sub[i]} --> {itemset_sub[0]} --- {round(confidence, 2)}\")\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "    ITEMS\n",
    "    1: Banana\n",
    "    2: Eggs\n",
    "    3: Milk\n",
    "    4: Tea\n",
    "    5: Bread\n",
    "\n",
    "\"\"\")\n",
    "f = {}\n",
    "\n",
    "for i in range(1, 5):\n",
    "    f = get_frequent_itemset(size=i, transactions=transactions,\n",
    "                             itemset=f)\n",
    "\n",
    "# frequent_itemsets\n",
    "\n",
    "print(\"Frequent Itemsets...\")\n",
    "for key, val in f.items():\n",
    "    print(f\"Itemset: {key}, support value: {val}\")\n",
    "\n",
    "\n",
    "subset = finding_subsets(f)\n",
    "\n",
    "for i in subset:\n",
    "    print(f\"Rules for itemset - {i[0]}\")\n",
    "    finding_rules(i)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
